{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import anomaly_plot\n",
    "from seq_data import *\n",
    "from utils import *\n",
    "\n",
    "# Disable file validation for PyDev debugger\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "\n",
    "# Set the multiprocessing start method to 'spawn'\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# Set the random seed to make the results reproducible\n",
    "set_seeds(313)\n",
    "\n",
    "# Set up the device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Set the data to name model\n",
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%Y_%m_%d_%H_%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Best Hyperparameters\n",
    "# python src/main.py --RAW_DIR G:/pdm/raw/ --MODEL_DIR G:/pdm/trained_models/ --MODEL_TYPE tfm --DATA_TYPE 2016 --TURBINE_ID T06 --CASE train --SYSTEM generator --BATCH_SIZE 128 --EPOCHS 500 --LEARNING_RATE 0.0001 --LEARNING unsup --SEQ_LEN 128 --HIDDEN_SIZE 256 --ENC_LAYERS 3 --DEC_LAYERS 3 --NUM_HEADS 8 --EMB_SIZE 64 --VAL_SPLIT 0.3 --DROPOUT 0.1 --LOSS_FN mse --OPTIMIZER rmsprop --SCHEDULER cosine --NUM_WORKERS 0\n",
    "\n",
    "\n",
    "class config:\n",
    "    def __init__(self):\n",
    "        self.RAW_DIR = Path(\"G:/turbine_ad/raw/\")\n",
    "        self.MODEL_DIR = Path(\"G:/turbine_ad/trained_models/\")\n",
    "        self.MODEL_TYPE = \"tfm\"  # incept\n",
    "        self.DATA_TYPE = \"2017\"  # 2016\n",
    "        self.TURBINE_ID = \"T07\"\n",
    "        self.CASE = \"train\"  # test\n",
    "        self.SYSTEM = \"gearbox\"  # generator\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.EPOCHS = 500\n",
    "        self.LEARNING_RATE = 0.001\n",
    "        self.LEARNING = \"unsup\"\n",
    "        self.SEQ_LEN = 128\n",
    "        self.HIDDEN_SIZE = 512\n",
    "        self.ENC_LAYERS = 4\n",
    "        self.DEC_LAYERS = 4\n",
    "        self.NUM_HEADS = 16\n",
    "        self.EMB_SIZE = 128\n",
    "        self.VAL_SPLIT = 0.3\n",
    "        self.DROPOUT = 0.1\n",
    "        self.LOSS_FN = \"mse\"\n",
    "        self.OPTIMIZER = \"adam\"\n",
    "        self.SCHEDULER = \"cosine\"\n",
    "        self.NUM_WORKERS = 4\n",
    "\n",
    "\n",
    "# Example usage of the config class\n",
    "args = config()\n",
    "args.RAW_DIR\n",
    "print(args.RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# from models.inception_unet import InceptionUNet\n",
    "# from models.ad_tfm import AD_TFM\n",
    "# InceptionUNet = InceptionUNet(in_channels=512, input_size=28, emb_size=64, kernel_size=None, stride=None, dropout=None)\n",
    "# AD_TFM = AD_TFM(d_model=28, emb_size=32, nhead=4, seq_len=512, num_encoder_layers=4,\n",
    "#                   num_decoder_layers=4, dim_feedforward=512//2, activation=nn.ReLU(), dropout=0.1,\n",
    "#                   norm_first=False)\n",
    "# input_tensor = torch.rand(32, 512, 28)  # Example input tensor with batch size=32, sequence length=100, and d_model=512\n",
    "# output = AD_TFM(input_tensor, input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory if it doesn't exist\n",
    "os.makedirs(args.MODEL_DIR, exist_ok=True)\n",
    "\n",
    "if args.LOSS_FN == \"mse\":\n",
    "    loss_fn = nn.MSELoss(reduction=\"mean\").to(device)\n",
    "elif args.LOSS_FN == \"hybrid\":\n",
    "    loss_fn = kl_mse_loss\n",
    "else:\n",
    "    raise ValueError(f\"Invalid loss function: {args.LOSS_FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.CASE == \"train\":\n",
    "\n",
    "    train_loaders, val_loaders, _, total_time = data_process(\n",
    "        args.RAW_DIR,\n",
    "        args.DATA_TYPE,\n",
    "        args.TURBINE_ID,\n",
    "        args.CASE,\n",
    "        args.SEQ_LEN,\n",
    "        args.BATCH_SIZE,\n",
    "        args.SYSTEM,\n",
    "        args.VAL_SPLIT,\n",
    "    )\n",
    "\n",
    "    batch_data = next(iter(val_loaders[0]))  # type: ignore\n",
    "    model_t = select_model(\n",
    "        model_type=args.MODEL_TYPE,\n",
    "        input_size=batch_data[0].shape[-1],\n",
    "        emb_size=args.EMB_SIZE,\n",
    "        seq_len=args.SEQ_LEN,\n",
    "        num_heads=args.NUM_HEADS,\n",
    "        hidden_size=args.HIDDEN_SIZE,\n",
    "        enc_layers=args.ENC_LAYERS,\n",
    "        dec_layers=args.DEC_LAYERS,\n",
    "        dropout=args.DROPOUT,\n",
    "        batch_size=args.BATCH_SIZE,\n",
    "        device=device,\n",
    "    )\n",
    "    model_t.apply(initialize_weights)\n",
    "\n",
    "    print(\n",
    "        f\"{args.MODEL_TYPE}:\",\n",
    "        sum(p.numel() for p in model_t.parameters() if p.requires_grad),\n",
    "    )\n",
    "\n",
    "    if args.OPTIMIZER == \"adam\":\n",
    "        optimizer = adam_optimizer(\n",
    "            model=model_t,\n",
    "            lr=args.LEARNING_RATE,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=0.,\n",
    "            amsgrad=True,\n",
    "        )\n",
    "    elif args.OPTIMIZER == \"rmsprop\":\n",
    "        optimizer = rmsprop_optimizer(\n",
    "            model=model_t,\n",
    "            lr=args.LEARNING_RATE,\n",
    "            alpha=0.99,\n",
    "            eps=1e-08,\n",
    "            weight_decay=1e-5,\n",
    "            momentum=0.9,\n",
    "            centered=False,\n",
    "        )\n",
    "    elif args.OPTIMIZER == \"sgd\":\n",
    "        optimizer = sgd_optimizer(\n",
    "            model=model_t,\n",
    "            lr=args.LEARNING_RATE,\n",
    "            momentum=0.9,\n",
    "            dampening=0,\n",
    "            weight_decay=0.,\n",
    "            nesterov=True,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer: {args.OPTIMIZER}\")\n",
    "\n",
    "    scheduler = scheduler_lr(\n",
    "        type=args.SCHEDULER,\n",
    "        optimizer=optimizer,\n",
    "        max_lr=args.LEARNING_RATE,\n",
    "        min_lr=args.LEARNING_RATE / 100,\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=15, min_delta=0.000001)  # type: ignore\n",
    "\n",
    "    writer = SummaryWriter(\n",
    "        comment=f\"_model{args.MODEL_TYPE}_data_{args.DATA_TYPE}_seqlen{args.SEQ_LEN}\"\n",
    "    )\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    model_result = train_and_evaluate(\n",
    "        model=model_t,\n",
    "        train_loaders=train_loaders,\n",
    "        val_loaders=val_loaders,\n",
    "        criterions=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        learning=args.LEARNING,\n",
    "        num_epochs=args.EPOCHS,\n",
    "        device=device,\n",
    "        scheduler=scheduler,\n",
    "        scheduler_type=args.SCHEDULER,\n",
    "        es=es,\n",
    "        tensorboard_writer=writer,\n",
    "    )\n",
    "\n",
    "    # End the timer and print out how long it took\n",
    "    end_time = timer()\n",
    "    print(f\"[INFO] Total training time: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "    save_name = (\n",
    "        f\"{date_string}_model{args.MODEL_TYPE}_data{args.DATA_TYPE}_tubineid{args.TURBINE_ID}_\"\n",
    "        f\"system{args.SYSTEM}_valsplit{args.VAL_SPLIT}_batchsize{args.BATCH_SIZE}_seqlen{args.SEQ_LEN}\"\n",
    "        f\"_hiddensize{args.HIDDEN_SIZE}_enclayers{args.ENC_LAYERS}_declayers{args.DEC_LAYERS}_embsize{args.EMB_SIZE}\"\n",
    "        f\"_numheads{args.NUM_HEADS}_lrate{args.LEARNING_RATE}_dropout{args.DROPOUT}.pth\"\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    save_model(\n",
    "        model=model_t,\n",
    "        results=model_result,\n",
    "        target_dir=f\"{args.MODEL_DIR}/{args.DATA_TYPE}/\",\n",
    "        model_name=save_name,\n",
    "        time=end_time - start_time,\n",
    "    )\n",
    "\n",
    "    # save models_results file as csv\n",
    "    df = pd.DataFrame(data=model_result)\n",
    "    _dir = f\"{str(args.RAW_DIR)[:-4]}/results/logs/\"\n",
    "    os.makedirs(_dir, exist_ok=True)\n",
    "    df.to_csv(f\"{_dir}{save_name[:-4]}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.CASE == \"test\":\n",
    "\n",
    "    tm = \"latest\"\n",
    "    model_dir = f\"{args.MODEL_DIR}/{args.DATA_TYPE}/\"\n",
    "    model_names = list(Path(model_dir).glob(\"*.pth\"))\n",
    "    model_names.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "\n",
    "    results = {\n",
    "        \"batch_size\": [],\n",
    "        \"seq_len\": [],\n",
    "        \"hidden_size\": [],\n",
    "        \"layers\": [],\n",
    "        \"n_heads\": [],\n",
    "        \"l_rate\": [],\n",
    "        \"dropout\": [],\n",
    "        \"RMSE\": [],\n",
    "        \"MAE\": [],\n",
    "    }\n",
    "    for model_name in model_names:\n",
    "        model_name = model_name.name\n",
    "        model_parameters = model_name.split(\"_\")\n",
    "        model = model_parameters[5][5:]\n",
    "        data_type = model_parameters[6][4:]\n",
    "        turbine_id = model_parameters[7][8:]\n",
    "        system = model_parameters[8][6:]\n",
    "        l_rate = model_parameters[-2][5:]\n",
    "        dropout = model_parameters[-1][7:-4]\n",
    "        val_split, b_size, s_len, h_size, n_layers, s_layers, e_size, n_heads = (\n",
    "            extracting_parameters(model_parameters[9:-2])\n",
    "        )\n",
    "\n",
    "        test_loaders, train_loaders, test_time, train_time = data_process(\n",
    "            args.RAW_DIR, data_type, turbine_id, args.CASE, s_len, b_size, system, val_split\n",
    "        )\n",
    "\n",
    "        batch_data = next(iter(test_loaders[0]))  # type: ignore\n",
    "        model_t = select_model(\n",
    "            model_type=model,\n",
    "            input_size=batch_data[0].shape[-1],\n",
    "            emb_size=e_size,\n",
    "            seq_len=s_len,  # type: ignore\n",
    "            hidden_size=h_size,\n",
    "            enc_layers=n_layers,\n",
    "            dec_layers=s_layers,\n",
    "            num_heads=n_heads,\n",
    "            dropout=0.0,\n",
    "            batch_size=batch_data[0].shape[0],\n",
    "            device=device,\n",
    "        )  # type: ignore\n",
    "\n",
    "        model_t = load_model(\n",
    "            model_t, model_dir, trained_model_name=model_name, device=device\n",
    "        )\n",
    "\n",
    "        system_features = selected_features(system)\n",
    "\n",
    "        if tm == \"all\":\n",
    "\n",
    "            output_dic, train_loss = infer_recon(model_t, train_loaders, device)\n",
    "\n",
    "            for i in range(10):\n",
    "                tack_time = train_time[i * s_len : (i + 1) * s_len]\n",
    "                plot_name = \"_\".join(model_parameters[:8])\n",
    "                plot_output_recon(\n",
    "                    output_dic['inputs'], output_dic['outputs'], i, system, system_features, tack_time, plot_name\n",
    "                )\n",
    "\n",
    "            # Get reconstruction loss threshold.\n",
    "            threshold = np.percentile(train_loss, 99)\n",
    "            print(\"Reconstruction error threshold: \", threshold)\n",
    "\n",
    "            start_time = timer()\n",
    "            output_dic, test_loss = infer_recon(model_t, test_loaders, device)\n",
    "            end_time = timer()\n",
    "\n",
    "            print(\n",
    "                f\"[INFO] Total inference time: {end_time - start_time:.3f} seconds\"\n",
    "            )\n",
    "\n",
    "            # Detect all the samples which are anomalies.\n",
    "            anomalies = test_loss > threshold\n",
    "            anomalies = np.any(anomalies, axis=1)\n",
    "            print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "            print(\"Indices of anomaly samples: \", np.where(anomalies)[0])\n",
    "            anomalies_indices = np.where(anomalies)[0]\n",
    "\n",
    "            ###### NOTE NOTE NOTE ######\n",
    "            ###### NOTE NOTE NOTE ######\n",
    "            # failures = pd.read_csv(args.RAW_DIR / \"T06_failures_2017.csv\")\n",
    "            # matching_rows = failures[failures['Component'].str.lower().str.contains(system.lower(), case=False, na=False)]\n",
    "            # failure_timestamps = matching_rows['Timestamp']\n",
    "\n",
    "            # failure_timestamps = pd.to_datetime(failure_timestamps)\n",
    "            # step_size = test_time.shape[0] // test_data_len # type: ignore\n",
    "            # ind = np.arange(0, test_time.shape[0], step_size) # type: ignore\n",
    "            # _test_time = test_time[ind] # type: ignore\n",
    "\n",
    "\n",
    "            # indx = []\n",
    "            # for failure_timestamp in failure_timestamps:\n",
    "            #     # Find the closest time before the failure timestamp\n",
    "            #     closest_time_before_failure = _test_time[test_time < failure_timestamp].max() # type: ignore\n",
    "\n",
    "            #     # Find the index of the closest time\n",
    "            #     index = np.where(_test_time == closest_time_before_failure)[0]\n",
    "            #     if len(index) > 0:\n",
    "            #         indx.append(index[0])\n",
    "\n",
    "\n",
    "            # n = 5\n",
    "            # for i in range(0, n*s_len, 16):\n",
    "            #     temp = indx[0] - n*s_len + i\n",
    "            #     time_stamp = test_time[indx[0] - n*s_len + i : indx[0] - (n-1)*s_len + i] # type: ignore\n",
    "            #     plot_name = \"_\".join(model_parameters[:8])\n",
    "            #     anomaly_plot(output_dic['inputs'], output_dic['outputs'], temp, system_features, time_stamp, plot_name)\n",
    "            ###### NOTE NOTE NOTE ######\n",
    "            ###### NOTE NOTE NOTE ######\n",
    "\n",
    "            output_dic[\"anomalies\"] = anomalies\n",
    "            for i in anomalies_indices:\n",
    "                time_stamp = test_time[i * s_len : ((i + 1) * s_len)] # type: ignore\n",
    "                plot_name = \"_\".join(model_parameters[:8])\n",
    "                anomaly_plot(output_dic['inputs'], output_dic['outputs'], i, system, system_features, time_stamp, plot_name)\n",
    "\n",
    "            # Save the results\n",
    "            os.makedirs(f\"{os.getcwd()}/results/{system}/anomalies/\", exist_ok=True)\n",
    "            np.savetxt(f\"{os.getcwd()}/results/{system}/anomalies/{model_name[:-4]}_indices.csv\", anomalies_indices, delimiter=\",\")\n",
    "\n",
    "            features = selected_features(system)\n",
    "            inputs = pd.DataFrame(output_dic[\"inputs\"].reshape(-1, len(selected_features(system))), columns=features)\n",
    "            outputs = pd.DataFrame(output_dic[\"outputs\"].reshape(-1, len(selected_features(system))), columns=features)\n",
    "            time_stamps = pd.Series(test_time[:inputs.shape[0]]) # type: ignore\n",
    "\n",
    "            input_df = pd.concat((time_stamps, inputs), axis=1)\n",
    "            columns = [\"Timestamp\", *features]\n",
    "            input_df.columns = columns\n",
    "\n",
    "            save_dir = f\"{os.getcwd()}/results/{data_type}/preds/{system}/\"\n",
    "            os.makedirs(f\"{save_dir}\", exist_ok=True)\n",
    "\n",
    "            input_df.to_csv(\n",
    "                f\"{save_dir}/{model_name[:-4]}_{tm}_input.csv\",\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "            recon_df = pd.concat((time_stamps, outputs), axis=1)\n",
    "            recon_df.columns = columns\n",
    "\n",
    "            recon_df.to_csv(\n",
    "                f\"{save_dir}/{model_name[:-4]}_{tm}_recon.csv\",\n",
    "                index=False,\n",
    "            )\n",
    "            break\n",
    "\n",
    "        \n",
    "        output_dic, train_loss = infer_recon(model_t, train_loaders, device)\n",
    "\n",
    "        for i in range(5):\n",
    "            tack_time = train_time[i * s_len : (i + 1) * s_len]\n",
    "            plot_name = \"_\".join(model_parameters[:8])\n",
    "            plot_output_recon(\n",
    "                output_dic['inputs'], output_dic['outputs'], i, system, system_features, tack_time, plot_name\n",
    "            )\n",
    "\n",
    "        # Get reconstruction loss threshold.\n",
    "        threshold = np.percentile(train_loss, 99)\n",
    "        print(\"Reconstruction error threshold: \", threshold)\n",
    "\n",
    "        start_time = timer()\n",
    "        output_dic, test_loss = infer_recon(model_t, test_loaders, device)\n",
    "        end_time = timer()\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Total inference time: {end_time - start_time:.3f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Detect all the samples which are anomalies.\n",
    "        anomalies = test_loss > threshold\n",
    "        anomalies = np.any(anomalies, axis=1)\n",
    "        print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "        print(\"Indices of anomaly samples: \", np.where(anomalies)[0])\n",
    "        anomalies_indices = np.where(anomalies)[0]\n",
    "\n",
    "\n",
    "        # Save the results\n",
    "        # os.makedirs(f\"{os.getcwd()}/results/{system}/anomalies/\", exist_ok=True)\n",
    "        # np.savetxt(f\"{os.getcwd()}/results/{system}/anomalies/{model_name[:-4]}_indices.csv\", anomalies_indices, delimiter=\",\")\n",
    "\n",
    "        features = selected_features(system)\n",
    "        inputs = pd.DataFrame(output_dic[\"inputs\"].reshape(-1, len(selected_features(system))), columns=features)\n",
    "        outputs = pd.DataFrame(output_dic[\"outputs\"].reshape(-1, len(selected_features(system))), columns=features)\n",
    "        time_stamps = pd.Series(test_time[:inputs.shape[0]]) # type: ignore\n",
    "\n",
    "        input_df = pd.concat((time_stamps, inputs), axis=1)\n",
    "        columns = [\"Timestamp\", *features]\n",
    "        input_df.columns = columns\n",
    "\n",
    "        save_dir = f\"{os.getcwd()}/results/{data_type}/preds/{system}/\"\n",
    "        os.makedirs(f\"{save_dir}\", exist_ok=True)\n",
    "\n",
    "        input_df.to_csv(\n",
    "            f\"{save_dir}/{model_name[:-4]}_{tm}_input.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        recon_df = pd.concat((time_stamps, outputs), axis=1)\n",
    "        recon_df.columns = columns\n",
    "\n",
    "        recon_df.to_csv(\n",
    "            f\"{save_dir}/{model_name[:-4]}_{tm}_recon.csv\",\n",
    "            index=False,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
